{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract examples\n",
    "\n",
    "https://ai.plainenglish.io/harnessing-the-openai-api-with-langchain-and-pydantic-for-structured-data-extraction-30e3e6966699\n",
    "\n",
    "By leveraging Langchain with Pydantic, developers and businesses alike can enhance their AI-powered applications, making them more robust and efficient. The Pydantic output parser is indeed a powerful tool that unlocks new potentials for structured data extraction from LLMs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = \"\"\"Large language models (LLMs) have demonstrated remarkable \\\n",
    "generalizability, such as understanding arbitrary entities and relations. \\\n",
    "Instruction tuning has proven effective for distilling LLMs \\\n",
    "into more cost-efficient models such as Alpaca and Vicuna. \\\n",
    "Yet such student models still trail the original LLMs by \\\n",
    "large margins in downstream applications. In this paper, \\\n",
    "we explore targeted distillation with mission-focused instruction \\\n",
    "tuning to train student models that can excel in a broad application \\\n",
    "class such as open information extraction. Using named entity \\\n",
    "recognition (NER) for case study, we show how ChatGPT can be distilled \\\n",
    "into much smaller UniversalNER models for open NER. For evaluation,\\\n",
    "we assemble the largest NER benchmark to date, comprising 43 datasets \\\n",
    "across 9 diverse domains such as biomedicine, programming, social media, \\\n",
    "law, finance. Without using any direct supervision, UniversalNER \\\n",
    "attains remarkable NER accuracy across tens of thousands of entity \\\n",
    "types, outperforming general instruction-tuned models such as Alpaca \\\n",
    "and Vicuna by over 30 absolute F1 points in average. With a tiny \\\n",
    "fraction of parameters, UniversalNER not only acquires ChatGPT's \\\n",
    "capability in recognizing arbitrary entity types, but also \\\n",
    "outperforms its NER accuracy by 7-9 absolute F1 points in average. \\\n",
    "Remarkably, UniversalNER even outperforms by a large margin \\\n",
    "state-of-the-art multi-task instruction-tuned systems such as \\\n",
    "InstructUIE, which uses supervised NER examples. \\\n",
    "We also conduct thorough ablation studies to assess the impact of \\\n",
    "various components in our distillation approach. We will release \\\n",
    "the distillation recipe, data, and UniversalNER models to facilitate \\\n",
    "future research on targeted distillation.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "from langchain.chains import create_extraction_chain\n",
    "from langchain_openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input': \"Large language models (LLMs) have demonstrated remarkable generalizability, such as understanding arbitrary entities and relations. Instruction tuning has proven effective for distilling LLMs into more cost-efficient models such as Alpaca and Vicuna. Yet such student models still trail the original LLMs by large margins in downstream applications. In this paper, we explore targeted distillation with mission-focused instruction tuning to train student models that can excel in a broad application class such as open information extraction. Using named entity recognition (NER) for case study, we show how ChatGPT can be distilled into much smaller UniversalNER models for open NER. For evaluation,we assemble the largest NER benchmark to date, comprising 43 datasets across 9 diverse domains such as biomedicine, programming, social media, law, finance. Without using any direct supervision, UniversalNER attains remarkable NER accuracy across tens of thousands of entity types, outperforming general instruction-tuned models such as Alpaca and Vicuna by over 30 absolute F1 points in average. With a tiny fraction of parameters, UniversalNER not only acquires ChatGPT's capability in recognizing arbitrary entity types, but also outperforms its NER accuracy by 7-9 absolute F1 points in average. Remarkably, UniversalNER even outperforms by a large margin state-of-the-art multi-task instruction-tuned systems such as InstructUIE, which uses supervised NER examples. We also conduct thorough ablation studies to assess the impact of various components in our distillation approach. We will release the distillation recipe, data, and UniversalNER models to facilitate future research on targeted distillation.\", 'text': [{'research_topic': 'Large language models', 'finding': 'demonstrated remarkable generalizability'}, {'research_topic': 'Instruction tuning', 'finding': 'proven effective for distilling LLMs into more cost-efficient models such as Alpaca and Vicuna'}, {'research_topic': 'Targeted distillation with mission-focused instruction tuning', 'finding': 'explore to train student models that can excel in a broad application class such as open information extraction'}, {'research_topic': 'Named entity recognition (NER)', 'finding': 'show how ChatGPT can be distilled into much smaller UniversalNER models for open NER'}, {'research_topic': 'NER benchmark', 'finding': 'assemble the largest NER benchmark to date, comprising 43 datasets across 9 diverse domains such as biomedicine, programming, social media, law, finance'}, {'research_topic': 'UniversalNER', 'finding': \"attains remarkable NER accuracy across tens of thousands of entity types, outperforming general instruction-tuned models such as Alpaca and Vicuna by over 30 absolute F1 points in average. With a tiny fraction of parameters, UniversalNER not only acquires ChatGPT's capability in recognizing arbitrary entity types, but also outperforms its NER accuracy by 7-9 absolute F1 points in average. Remarkably, UniversalNER even outperforms by a large margin state-of-the-art multi-task instruction-tuned systems such as InstructUIE, which uses supervised NER examples\"}, {'research_topic': 'Ablation studies', 'finding': 'conduct thorough ablation studies to assess the impact of various components in our distillation approach'}]}\n"
     ]
    }
   ],
   "source": [
    "# Extraction using OpenAI functions\n",
    "# Schema which will be filled using extracted information\n",
    "\n",
    "schema = {\n",
    "    \"properties\": {\n",
    "        \"research_topic\": {\"type\": \"string\"},\n",
    "        \"problem_statement\": {\"type\": \"string\"},\n",
    "        \"experiment_design\": {\"type\": \"string\"},\n",
    "        \"finding\": {\"type\": \"string\"},\n",
    "    },\n",
    "    \"required\": [\"research_topic\", \"finding\"],\n",
    "}\n",
    "\n",
    "llm = ChatOpenAI(temperature=0, model=\"gpt-3.5-turbo\")\n",
    "chain = create_extraction_chain(schema, llm)\n",
    "response = chain.invoke(inp)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2: Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "\n",
    "class Person(BaseModel):\n",
    "    \"\"\"Information about a person.\"\"\"\n",
    "\n",
    "    # ^ Doc-string for the entity Person.\n",
    "    # This doc-string is sent to the LLM as the description of the schema Person,\n",
    "    # and it can help to improve extraction results.\n",
    "\n",
    "    # Note that:\n",
    "    # 1. Each field is an `optional` -- this allows the model to decline to extract it!\n",
    "    # 2. Each field has a `description` -- this description is used by the LLM.\n",
    "    # Having a good description can help improve extraction results.\n",
    "    name: Optional[str] = Field(default=None, description=\"The name of the person\")\n",
    "    hair_color: Optional[str] = Field(\n",
    "        default=None, description=\"The color of the person's hair if known\"\n",
    "    )\n",
    "    height_in_meters: Optional[str] = Field(\n",
    "        default=None, description=\"Height measured in meters\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "# Define a custom prompt to provide instructions and any additional context.\n",
    "# 1) You can add examples into the prompt template to improve extraction quality\n",
    "# 2) Introduce additional parameters to take context into account (e.g., include metadata\n",
    "#    about the document from which the text was extracted.)\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are an expert extraction algorithm. \"\n",
    "            \"Only extract relevant information from the text. \"\n",
    "            \"If you do not know the value of an attribute asked to extract, \"\n",
    "            \"return null for the attribute's value.\",\n",
    "        ),\n",
    "        # Please see the how-to about improving performance with\n",
    "        # reference examples.\n",
    "        # MessagesPlaceholder('examples'),\n",
    "        (\"human\", \"{text}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/langchain_core/_api/beta_decorator.py:87: LangChainBetaWarning: The method `ChatMistralAI.with_structured_output` is in beta. It is actively being worked on, so the API may change.\n",
      "  warn_beta(\n"
     ]
    }
   ],
   "source": [
    "from langchain_mistralai import ChatMistralAI\n",
    "\n",
    "llm = ChatMistralAI(model=\"mistral-large-latest\", temperature=0)\n",
    "\n",
    "runnable = prompt | llm.with_structured_output(schema=Person)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Person(name='Alan Smith', hair_color='blond', height_in_meters='1.8288')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"Alan Smith is 6 feet tall and has blond hair.\"\n",
    "runnable.invoke({\"text\": text})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "autogen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
