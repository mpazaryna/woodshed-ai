{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import find_dotenv, load_dotenv\n",
    "load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_groq import ChatGroq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chat = ChatGroq(temperature=0, model_name=\"llama3-8b-8192\")\n",
    "# chat = ChatGroq(temperature=0, model_name=\"llama3-70b-8192\")\n",
    "chat = ChatGroq(temperature=0, model_name=\"gemma-7b-it\")\n",
    "# chat = ChatGroq(temperature=0, model_name=\"mixtral-8x7b-32768\")\n",
    "# chat = ChatGroq(temperature=0, model_name=\"whisper-large-v3\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Importance of Low Latency LLMs:**\n",
      "\n",
      "**1. Real-time Applications:**\n",
      "\n",
      "* Low latency LLMs enable real-time applications such as conversational assistants, language translation, and question-answering systems.\n",
      "* By reducing response times, users can interact with LLMs seamlessly and efficiently.\n",
      "\n",
      "\n",
      "**2. Improved User Experience:**\n",
      "\n",
      "* Fast response times enhance the overall user experience by providing immediate feedback and reducing waiting time.\n",
      "* This is especially crucial for interactive and conversational applications.\n",
      "\n",
      "\n",
      "**3. Enhanced Efficiency and Productivity:**\n",
      "\n",
      "* By reducing latency, LLMs can process and generate responses faster, leading to increased efficiency and productivity.\n",
      "* This is particularly beneficial for tasks involving natural language processing and content generation.\n",
      "\n",
      "\n",
      "**4. Seamless Integration:**\n",
      "\n",
      "* Low latency LLMs can be seamlessly integrated into existing applications and workflows without significant performance overhead.\n",
      "* This allows for a smooth and uninterrupted user experience.\n",
      "\n",
      "\n",
      "**5. Continuous Interaction:**\n",
      "\n",
      "* By minimizing latency, LLMs can facilitate continuous interaction between users and the system.\n",
      "* This is essential for applications that require ongoing conversations or continuous content generation.\n",
      "\n",
      "\n",
      "**6. Global Accessibility:**\n",
      "\n",
      "* Low latency LLMs can provide access to language processing capabilities in remote or geographically distributed locations.\n",
      "* This promotes inclusivity and accessibility to users worldwide.\n",
      "\n",
      "\n",
      "**7. Real-time Decision-making:**\n",
      "\n",
      "* In applications where quick decisions are required, low latency LLMs can provide timely insights and recommendations.\n",
      "* This is valuable for industries such as healthcare, finance, and transportation.\n",
      "\n",
      "\n",
      "**8. Enhanced Creativity and Inspiration:**\n",
      "\n",
      "* By reducing latency, LLMs can provide inspiration and creative ideas in real-time.\n",
      "* This is beneficial for writers, artists, and innovators.\n",
      "\n",
      "\n",
      "**9. Improved Search Results:**\n",
      "\n",
      "* Low latency LLMs can enhance search engine results by providing relevant and concise summaries.\n",
      "* This improves the overall search experience for users.\n",
      "\n",
      "\n",
      "**10. Future-proofing Technology:**\n",
      "\n",
      "* As latency becomes increasingly important in AI, low latency LLMs will become a foundational technology for future applications and innovations.\n"
     ]
    }
   ],
   "source": [
    "system = \"You are a helpful assistant.\"\n",
    "human = \"{text}\"\n",
    "prompt = ChatPromptTemplate.from_messages([(\"system\", system), (\"human\", human)])\n",
    "\n",
    "chain = prompt | chat | StrOutputParser()\n",
    "ouput = chain.invoke({\"text\": \"Explain the importance of low latency LLMs.\"})\n",
    "print(ouput)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The golden orb ascends the sky,\\nCasting shadows long and nigh.\\nIt warms the land and sea,\\nAs stars begin to flee,\\nThe Sun, a beacon, burning nigh.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = ChatPromptTemplate.from_messages([(\"human\", \"Write a Limerick about {topic}\")])\n",
    "chain = prompt | chat | StrOutputParser()\n",
    "await chain.ainvoke({\"topic\": \"The Sun\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silver crescent glow\n",
      "Dancing in the midnight sky\n",
      "Luna's gentle beam"
     ]
    }
   ],
   "source": [
    "prompt = ChatPromptTemplate.from_messages([(\"human\", \"Write a haiku about {topic}\")])\n",
    "chain = prompt | chat \n",
    "for chunk in chain.stream({\"topic\": \"The Moon\"}):\n",
    "    print(chunk.content, end=\"\", flush=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cloudspace",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
